# -*- coding: utf-8 -*-
"""ml-hw3-01057162.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vLFGeoRBx5Cf_kJ1jUmfpBYxtviiZj1E

#Use Kaggle to get dataset
However, the heart csv has been provided, this step is ommitted.
"""

!pip install -q kaggle

"""#Import libraries"""

import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import pickle
import sklearn as sk
import os
from IPython.display import clear_output, display

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.ensemble import RandomForestClassifier

try:
  from treeinterpreter import treeinterpreter as ti
except:
  !pip install treeinterpreter
  from treeinterpreter import treeinterpreter as ti

from sklearn.tree import DecisionTreeClassifier, plot_tree

"""#Read CSV file."""

df = pd.read_csv('/content/heart.csv')

df[['sex','cp','fbs','restecg','exang','thal']].hist()
plt.tight_layout()

"""#Create training sets, validation sets and testing sets respectively."""

from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size = 0.2)
train, valid = train_test_split(train, test_size = 0.1)

train_x = train.copy()
train_y = train_x.pop('target')
valid_x = valid.copy()
valid_y = valid_x.pop('target')
test_x = test.copy()
test_y = test_x.pop('target')

pd.concat([train_x, train_y], axis = 1).head()
df.iloc[[80,59,186],:].head()

"""#Change data format in order to suit the needs for Classifier Training."""

fc = tf.feature_column
NUMERIC_COLUMNS = ['age','trestbps','chol','thalach','oldpeak','slope','ca']
BUCKETIZED_COLUMNS = [(feature_column.numeric_column('age', dtype = tf.float32), list(np.linspace(15,65,11)))]
CATEGORICAL_COLUMNS = ['sex','cp','fbs','restecg','exang']

EMBEDDING_COLUMNS = []

HASHED_COLUMNS = [('thal',100)]
CROSSED_COLUMNS = [(feature_column.bucketized_column(feature_column.numeric_column('age', dtype = tf.float32), list(np.linspace(15,65,11))),
                     feature_column.categorical_column_with_vocabulary_list('thal',train_x['thal'].unique(),dtype = tf.int32), 100)]

feature_columns = []

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(feature_column.numeric_column(feature_name,dtype = tf.float32))

for (nc, boundaries) in BUCKETIZED_COLUMNS:
  feature_columns.append(feature_column.bucketized_column(nc, boundaries = boundaries))

for feature_name in CATEGORICAL_COLUMNS:
  vocab = train_x[feature_name].unique()
  one_hot = feature_column.categorical_column_with_vocabulary_list(feature_name,vocab)
  feature_columns.append(feature_column.indicator_column(one_hot))

for (c_fc, dim) in EMBEDDING_COLUMNS:
  ec = feature_column.embedding_column(c_fc, dimension = dim)
  feature_columns.append(ec)

for (feature_name, hash_bucket_size) in HASHED_COLUMNS:
  hashed_bucket = feature_column.categorical_column_with_hash_bucket(feature_name, hash_bucket_size = hash_bucket_size, dtype = tf.int32)
  feature_columns.append(feature_column.indicator_column(hashed_bucket))

for (f1,f2, hash_bucket_size) in CROSSED_COLUMNS:
  crossed_feature = feature_column.crossed_column([f1,f2], hash_bucket_size = hash_bucket_size)
  feature_columns.append(feature_column.indicator_column(crossed_feature))

"""#Use tensorflow to create data input pipeline."""

def make_input_fn(X, y, n_epochs = None, shuffle = True):
  def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((dict(X),y))
    if shuffle:
      dataset = dataset.shuffle(len(y))
    dataset = dataset.repeat(n_epochs)
    dataset = dataset.batch(len(y))
    return dataset

  return input_fn

train_input_fn = make_input_fn(train_x, train_y)
valid_input_fn = make_input_fn(valid_x, valid_y)
test_input_fn = make_input_fn(test_x, test_y,1)

"""#Neural Network"""

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
model = tf.keras.Sequential([
    feature_layer,
    layers.Dense(128, activation = 'relu'),
    layers.Dense(128, activation = 'relu'),
    layers.Dense(1, activation = 'sigmoid')
])

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(train_input_fn(), epochs = 30, validation_data = valid_input_fn(), steps_per_epoch = 10, validation_steps = 10, verbose = 0)
results = model.evaluate(test_input_fn(), verbose = 0)
pd.Series({'loss': results[0], 'acc': results[1]}).to_frame()

tf.__version__

"""#Using BoostedTreesClassifier in tensorflow."""

params = {
    'n_trees' : 50,
    'max_depth' : 3,
    'n_batches_per_layer' : 1,
    'center_bias' : True
}

est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)

est.train(train_input_fn, max_steps = 100)

results = est.evaluate(test_input_fn)
clear_output()
pd.Series(results).to_frame()

"""#Data checking."""

pd.DataFrame({name:[v] for name, v in est.experimental_feature_importances().items()}).T

#check DFC of training module.
pred_dicts = list(est.experimental_predict_with_explanations(make_input_fn(train_x, train_y, 1)))

df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])
df_dfc.abs().describe().T.sort_values(['mean'], ascending = False)

#check DFC of sample module.
pred_dicts = list(est.experimental_predict_with_explanations(make_input_fn(test_input_fn)))

df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])
df_dfc.abs().describe().T.sort_values(['mean'], ascending = False)

#check DFC of sample.
pd.DataFrame([pred_dicts[2]['dfc']]).abs().head(1).T

"""#Using RandomForestClassifier in sklearn."""

rf_clf = RandomForestClassifier(random_state = 96)
rf_clf.fit(train_x, train_y)
rf_acc = rf_clf.score(test_x, test_y)
print("acc: {:.3f}".format(rf_acc))

"""#Data checking."""

pd.DataFrame({name:[cr] for cr, name in zip(rf_clf.feature_importances_, train_x.columns)}).head().T.sort_values([0], ascending = False)

#Check DFC for Training Module.

prediction, bias, contributions = ti.predict(rf_clf, train_x)
contrib = []
for p, b, c in zip(prediction, bias, contributions):
  class_id = np.argmax(p)
  contrib.append(np.abs(np.array(c)[:, class_id].ravel()))

pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.mean(np.array(contrib), axis = 0))}).head().T.sort_values([0], ascending = False)

#Check DFC for Testing Module.

prediction, bias, contributions = ti.predict(rf_clf, test_x)
contrib = []
for p, b, c in zip(prediction, bias, contributions):
  class_id = np.argmax(p)
  contrib.append(np.abs(np.array(c)[:, class_id].ravel()))

pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.mean(np.array(contrib), axis = 0))}).head().T.sort_values([0], ascending = False)

#Check DFC for Specified Module.

instance_id = 10
prediction, bias, contributions = ti.predict(rf_clf, test_x[instance_id: instance_id + 1])
class_id = np.argmax(prediction[0,:])
pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.abs(np.array(contributions[0,:, class_id])).ravel())}).head().T.sort_values([0],ascending = False)

"""#Using DecisionTreeClassifier from sklearn."""

dt_clf = DecisionTreeClassifier(random_state = 96)
dt_clf.fit(train_x,train_y)
dt_acc = dt_clf.score(test_x, test_y)
print('acc: {:.3f}'.format(dt_acc))

"""#Data checking.

dt_clf.importances will record every attribute in a record, this can reduce data uncertainty when building the Random Forest, attributes with a higher value has a higher importance.
"""

pd.DataFrame({name:[cr] for cr, name in zip(dt_clf.feature_importances_, train_x.columns)}).head().T.sort_values([0], ascending = False)

"""Display the structure of the Decision Tree."""

plt.figure(figsize = (20,20))
a = plot_tree(dt_clf, filled =True)
plt.savefig('decision_tree.pdf')

#Check DFC for Training Module.

prediction, bias, contributions = ti.predict(dt_clf, train_x)
contrib = []
for p, b, c in zip(prediction, bias, contributions):
    class_id = np.argmax(p)
    contrib.append(np.abs(np.array(c)[:,class_id].ravel()))

pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.mean(np.array(contrib),axis=0))}).head().T.sort_values([0],ascending=False)

#Check DFC for Sampling Module.

prediction, bias, contributions = ti.predict(dt_clf, test_x)
contrib = []
for p, b, c in zip(prediction, bias, contributions):
    class_id = np.argmax(p)
    contrib.append(np.abs(np.array(c)[:,class_id].ravel()))

pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.mean(np.array(contrib),axis=0))}).head().T.sort_values([0],ascending=False)

#Check DFC for a Specific Sample.

instance_id = 10
prediction, bias, contributions = ti.predict(dt_clf, test_x[instance_id:instance_id+1])
class_id = np.argmax(prediction[0,:])
pd.DataFrame({name:[cr] for name, cr in zip(test_x.columns, np.abs(np.array(contributions[0,:,class_id])).ravel())}).head().T.sort_values([0],ascending=False)

def extract_rules(clf,names=None):
    def dfs(node_id,precond):
        if clf.tree_.children_left[node_id]==-1 and clf.tree_.children_right[node_id]==-1: # a leaf node
            val = clf.tree_.value[node_id].ravel()
            pp  = val/np.sum(val)
            pp  = ','.join(['{:.3f}'.format(x) for x in pp])
            pr  = ' and '.join(precond)
            rules.append('if '+ pr +' then ' + 'class {}, '.format(np.argmax(val)) + 'posterior prob. ['+ pp +']')
            return
        if clf.tree_.children_left[node_id] >= 0:
            new_precond = precond.copy()
            new_precond.append('{}<={:.3f}'.format(names[clf.tree_.feature[node_id]],clf.tree_.threshold[node_id]))
            dfs(clf.tree_.children_left[node_id], new_precond)
        if clf.tree_.children_right[node_id] >= 0:
            new_precond = precond.copy()
            new_precond.append('{}>{:.3f}'.format(names[clf.tree_.feature[node_id]],clf.tree_.threshold[node_id]))
            dfs(clf.tree_.children_right[node_id], new_precond)
        return
#-------------------------------------------------
    if names is None:
        names = ['f_{}'.format(x) for x in range(clf.tree_.n_features)]

    rules = []
    precond=[]
    dfs(0,precond)
    return rules

extract_rules(dt_clf,train_x.columns)

"""#Using tf.data and sklearn's Random Forest Classifier and Decision Tree Classifier"""

fc_train_x = feature_layer(dict(train_x)).numpy()
fc_test_x  = feature_layer(dict(test_x)).numpy()

rf_clf.fit(fc_train_x,train_y)
rf_acc = rf_clf.score(fc_test_x,test_y)
print("RandomForest test accuracy:{:.3f}".format(rf_acc))

dt_clf.fit(fc_train_x,train_y)
dt_acc = dt_clf.score(fc_test_x,test_y)
print("DecisionTree test accuracy:{:.3f}".format(dt_acc))

"""#Conclusion  
  
Using Forest Classifiers can help in grouping data with similar attributes.  
Although I cannot run the Boosted Tree Classifier, I have learned how a Random Forest and Decision Tree could help in grouping data.
"""
